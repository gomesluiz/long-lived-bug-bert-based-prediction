
\section{Related Work}\label{chapter:4:sec:related}

This section overviews relevant related work. We retrieved these works from the ACM Digital Library,\footnote{\url{https://dl.acm.org/} (As of Jun. 2022).} the IEEE Xplore,\footnote{\url{https://ieeexplore.ieee.org/} (As of Jun. 2022)} ScienceDirect,\footnote{\url{https://www.sciencedirect.com/} (As of Jun. 2022).} and  Springer.\footnote{\url{https://www.springer.com/}(As of Jun. 2022)}. To find them, we constructed a search string using six main terms: ``software maintenance," ``bug report," ``bug fixing," ``bug resolution," and ``long-lived." The syntax of the string was adapted according to the source (e.g., wildcards, connectors, apostrophes, and quotation marks) and was then applied on the abstract of the manuscripts being searched. We searched databases for manuscripts published between 2010 and 2022.

Giger et al.~\cite{Giger:2010} conducted experiments on bug reports from six FLOSS projects hosted by Eclipse, Mozilla, and Gnome, and proposed a classifier based on a decision tree classifier to classify bugs into ``fast'' or ``slow.'' Furthermore, they empirically demonstrated that the addition of post-submission bug report data of up to one month in the feature vector might improve the model performance.   

Lamkanfi et al.~\cite{Lamkanfi:2010b} observed that a fraction of the conspicuous fix-times bug reports is often fixed within a few minutes in Eclipse and Mozilla. The authors proposed to filter out these conspicuous bug reports when using data mining techniques to predict the fixing times of reported bugs. 

Zhang et al.~\cite{Zhang:2013} performed an empirical study on bug fixing-times in three projects of CA Technologies company. They proposed a model based on a Markov chain to predict the number of bugs that could be fixed in the future. Also, they employed a Monte Carlo simulation to predict the total fixing time for a given amount of bugs. Moreover, they classified bugs as ``fast" and ``slow" regarding different threshold times. Akbarinasaji et al.~\cite{Akbarinasaji:2018} replicated the study performed by Zhang et al.~\cite{Zhang:2013}. Rather than a CSS project, Akbarinasaji et al.~\cite{Akbarinasaji:2018}  investigated an open-source software project and confirmed results shown by Zhang et al.~\cite{Zhang:2013}.

Saha et al.~\cite{Saha:2015b} extracted the bug repositories from seven well-known FLOSS projects and analyzed long-lived bugs from five different perspectives: proportion, severity, assignment, reasons, and the nature of fixes. Although less frequent than short-lived bugs, they showed a fair number of long-lived bugs in FLOSS projects, and more than 90\% of them negatively affected user experience. The reasons for these long-lived bugs are many, including, for example, longer assignment time and the lack of understanding of their priority. However, many bugs resulted in long-lived bugs without a specific reason.

Rocha et al.~\cite{Rocha:2016} characterized the workflow followed by Mozilla Firefox developers when resolving bugs. They proposed the concept of bug flow graphs (BFG) to help understand the workflow. They concluded that (a) when a bug is not formally assigned to a developer, it takes ten more days to be resolved; (b) approximately 94\% of duplicate bugs are resolved within two days or less after they appear in the tracking system; (c) incomplete bugs, which are never assigned to developers, usually take 70 days to be closed; (d) more skilled developers resolve bugs faster in comparison to less skilled ones; (e) for less skilled developers, assigning a person responsible for the bug usually takes more time in comparison to the time taken to fix the bug.

Habayeb et al.~\cite{Habayeb:2018} proposed a novel approach using Hidden Markov models and temporal sequences to predict when a bug report will be closed. The approach is empirically demonstrated using eight years of bug reports collected from the Firefox project. The results indicate around 10\% higher accuracy than the frequency-based classification approaches. 

Ardimento et al.~\cite{ardimento:2020} proposed a method based on BERT\textsubscript{BASE}
to predict bug-fixing time. Their approach combined description and comments attributes of a bug report to provide for the BERT\textsubscript{BASE} neural network. They evaluated their proposed model on Live Codee, a large-scale open-source project, and they claimed that their proposed approach has an effective ability to predict a bug in ``slow'' or ``fast.'' They used the median to label each bug report in the training dataset in ``slow'' and ``fast.''

Sepahvand et al.~\cite{Sepahvand:2020} proposed an approach based on Long Short-Term Memory~(LSTM) to classify a bug in short fixing time or long fixing time. The class of each bug report was determined by applying a threshold. Short-fixing time class is assigned for bugs with fixing time lesser than the threshold, and long-fixing time class for others. The threshold was the median of all bug fixing time extracted from Mozilla from 2008 to 2014). The results show that the proposed method had better performance than the hidden Markov-based~\cite{Habayeb:2018} model in the same task.

In our previous work~\cite{gomes:2019}, we investigated the population of long-lived bugs in six popular FLOSS. Also, we confirmed a significant percentage of long-lived bugs in these projects; we characterized them using many bug report attributes, which confirmed some differences between short- and long-lived bugs. Furthermore, we compared the accuracy of five well-known ML classifiers and traditional text mining techniques in long-lived bug prediction. Our experiments used unstructured text attributes and demonstrated that it is possible to predict long-lived bugs with good accuracy using basic methods.

There are broad research efforts toward the use of BERT and the effect of its contextual embedding in Software Engineering tasks: identifying correct patches~\cite{csuvik:2020},  searching and documenting code~\cite{feng:2020}, representing data flow~\cite{guo:2020}, summarizing code\cite{wang:2020} , predicting defects~\cite{akimova:2021}, detecting and repairing bug~\cite{allamanis:2021}, extracting software requirements~\cite{dearaujo:2021}, traceability of software artifacts~\cite{jinfeng:2021}, and localizing bugs~\cite{zou:2021}.

The studies above present relevant results for researchers in this area. However, we can observe shortcomings in some works that investigated ``long-lived prediction'' itself. First, studies often used a few ML classifiers: Decision Tree~\cite{ Giger:2010}, Markov Chain~\cite{ Zhang:2013}, LSTM~\cite{Sepahvand:2020}, and BERT~\cite{ardimento:2020}. Different from them, in this paper, we perform a more comprehensive investigation including different feature extractors and classifiers. We compare the accuracy of five well-known ML classifiers long-lived bug prediction using BERT and TF-IDF as feature extractors. Also,  we detail and discuss cases of success associated with the best predictors.

Furthermore, another limitation of those studies refer to the fact that they considered few FLOSS projects. In this paper, we consider six popular FLOSS projects (Eclipse, Freedesktop, Gnome, GCC, Mozilla, and WineHQ) in our evaluation protocol.


