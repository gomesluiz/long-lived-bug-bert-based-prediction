\section{Introduction}\label{chapter:4:sec:introduction}

Today's software maintenance activities in Free/Libre Open-Source Software~(FLOSS) and Closed Source Software~(CSS) rely mainly on information extracted from bug reports opened in a Bug Tracking Systems (BTS). This kind of system plays a key role as a communication and collaboration tool in both environments. However, with many users and developers with different expertise and spread out worldwide, FLOSS increased the requirement of using a BTS in the maintenance pipeline.

Users and developers interact with the maintenance team filling out a brief description, a long description, and provisional severity level associated with a bug in a bug report form provided by a BTS. Next, a maintenance team member  reviews the bug report and either approve or reject it. If a team member approved the bug report, he or she provides further information, such as indicating its priority and assigning a person in charge of fixing it. Due to the high number of bug reports in medium- and large-size FLOSS projects, the manual handling of bug reports may be entirely subjective, tiresome, and error-prone~\cite{Lamkanfi:2010, Lamkanfi:2011, Yang:2017}.\done{ISSUES: 1.6 - support this statement} Therefore, a wrong decision within the bug report lifecycle may strongly affect the planning of maintenance activities. 

Estimating fixing times and the immediate identification of opened bugs that may have a long-lived lifecycle are essential for maintenance teams to build their plan. With that, managers may improve the resource allocation and better create a release roadmap, avoiding the bugs backlog overgrow~\cite{Abdelmoez:2012, Zhang:2013, Ardimento:2016}. Furthermore, the correct prediction of long-lived bugs may help the quality assurance team improve their daily activities~\cite{Ardimento_Dinapoli:2017, Abbood:2017, Akbarinasaji:2018, Sepahvand:2020}.\done{ISSUES: 1.7 - support this statement} They could fix more bugs that often adversely affect software quality and disturb the user experience across versions of FLOSS, which may increase customers' satisfaction, avoiding them to switch in favor of a competitor~\cite{Rawal:2015, Saha:2015b, Mezouar:2018, Akbarinasaji:2018}.

In the literature, despite its importance, it seems there is not a common understanding about what is a long-live bug\cite{saha:2014, Saha:2015, Saha:2015b,gomes:2021}. While some authors~\cite{Canfora:2011, Marks:2011, saha:2014, Saha:2015b} consider absolute values as thresholds (based on the release cycle), others~\cite{Abdelmoez:2012, Giger:2010, Francis_Williams:2013, Ardimento:2016, Ardimento_Dinapoli:2017, Sepahvand:2020} consider threshold values based on the statistical distribution of bug-fixing times for each FLOSS project. \done{ISSUES: 1.5 randomizing categorization of bugs into short-lived and long-lived}. These different visions may suggest that the definition of the long-live threshold is related to particular characteristics of each project (e.g., number of people in the team).

Machine Learning~(ML) and Text Mining~(TM) techniques have solved many real-world prediction problems, including those related to automating bug report handling, such as bug severity prediction~\cite{Singh:2017, Roy:2017, gomes:2019, Saha:2015b, Rocha:2016, gomes:2021}. Advances in Natural Language Processing~(NLP) have shifted the research focus from traditional to deep-learning-based techniques. Bidirectional Encoder Representations from Transformers (BERT), a deep neural learning network for NLP, have recently surpassed classical text mining approaches in various text classification tasks~\cite{Devlin:2019, Gonzalez:2020, sun:2020}. There are two approaches to adapt BERT for particular tasks: feature extraction and fine-tuning. The first method freezes model weights, and the pre-trained representations are used in a downstream model like standard feature-based approaches. In the second method, in turn, the pre-trained model can be unfrozen and fine-tuned on a new task~\cite{Peters:2019}.

\done{ISSUES: 2.2, 2.3: cite other papers that investigate long-lived bugs and better motivate our research}
There are broad research efforts toward the use of BERT and the effect of its contextual embedding in Software Engineering tasks~\cite{csuvik:2020, feng:2020, guo:2020, kanade:2020, wang:2020, akimova:2021, allamanis:2021, dearaujo:2021, jinfeng:2021, wang:2021, zou:2021}. Even so, to the best of our knowledge, few studies applied it in long-lived bug prediction task~\cite{ardimento:2020}. Ardimento et al.~\cite{ardimento:2020}, for instance, yielded relevant results in their experiment using fine-tuning in this prediction task. However, they carried out their investigation on only one dataset extracted from LiveCode BTS with a relatively small number of bug reports.  Still, performed experiments described in their paper were not deeply investigated. 

In this paper, we investigate both using of BERT-based feature extractor and fine-tuning BERT approach for long-lived bug prediction. Furthermore, we compared the accuracy performance of model predictors on six popular FLOSS projects, using feature extraction, fine-tuning based on BERT or TF-IDF. 

\begin{quote}
  {\it Evaluate the long-lived prediction accuracy of five well-known machine learning classifiers when using BERT and TF-IDF as feature extractor or BERT fine-tuning}
\end{quote}

This specific goal leads to the definition of the following Research Questions (RQs) addressed in our study:

\begin{enumerate}[label=\textbf{RQ$_{\arabic*}$}.]\addtocounter{enumi}{0}

    \item {\it What is the comparative accuracy of ML classifiers when predicting long-lived bugs using BERT-based feature extraction?}~We aim to compare the accuracy of long-lived bug prediction systems based on different classifiers when BERT features are used. Answering this research question is essential for understanding the impact of contextual embedding features provided by BERT on the accuracy of traditional ML classifiers.
    
    \item {\it What is the comparative accuracy of ML classifiers when predicting long-lived bugs using BERT-based feature extraction or TF-IDF-based feature extraction?}~We aim to compare the accuracy of long-lived bug prediction systems when different feature extractors, based on BERT and TF-IDF, are employed. Answering this research question provides insights related to the choice of suitable features for automating the prediction of long-lived bugs with high accuracy.
    
    \item {\it  What is the comparative accuracy BERT fine-tuning approach and ML algorithms using TF-IDF when predicting long-lived bugs TF-IDF-based feature extraction?}. \todo{We aim to compare the accuracy of long-lived bug prediction systems when different feature extractors, based on BERT and TF-IDF, are employed. Answering this research question provides insights related to the choice of suitable features for automating the prediction of long-lived bugs with high accuracy}
  
\end{enumerate}

\todo{ISSUES: 1.1 - lacking of novelty compared to previous work; 3.2 - The paper's formulation of the classification problem is merely an application of text classification to bug reports.}
The contributions of this paper can be summarized as follows:
\begin{itemize}
  \item A quantitative assessment of the performance of traditional ML classifiers using features extracted with BERT for the long-lived bug prediction;
  \item A quantitative performance comparison between BERT and TF-IDF for extracting features on the long-lived prediction task; and
  \item Discussion upon qualitative characteristics associated with bugs correctly and incorrectly classified as long-lived.
\end{itemize}

We organized the paper as follows.  
Section~\ref{chapter:4:sec:terminology} provides background concepts related to bug tracking systems, text mining, and machine learning techniques. Section~\ref{chapter:4:sec:related} presents related work. Section~\ref{chapter:4:sec:methodology} describes the methodology used. Section~\ref{chapter:4:sec:results} reports our results. Section~\ref{chapter:4:sec:discussions} describes the significance of our findings and how they can be interpreted. Section \ref{chapter:4:sec:validity} describes the main threats to the validity of our research. Finally, Section~\ref{chapter:4:sec:conclusion} states our conclusions and highlights possible future research associated with our findings.