\section{Conclusions}\label{chapter:4:sec:conclusion}

Our paper investigated the impact of BERT-based feature extraction on the accuracy of ML classifiers in contrast with TF-IDF in long-lived prediction tasks. We used six well-known ML classifiers: KNN, NÃ¤ive Bayes, Neural Network, Random Forest, and SVM. The datasets used in our experiments were built from bug reports extracted from six popular datasets: Eclipse, Freedesktop, Gnome, Gcc, Mozilla, and WineHQ.

The results indicated that the accuracy of ML classifiers using BERT-based feature extraction, considering only the description attribute, was very promising. The SVM and Random Forest outperform others in almost all datasets~($RQ_{1}$). In comparison, the performance of ML classifiers when they used feature extraction based on BERT was systematically better than feature extraction based on TF-IDF. The highest accuracy difference occurred in Mozilla and the lowest in the Gnome project~($RQ_{2}$).

\done{Finally, Findings of ~($RQ_{3}$) shown that fine-tuning on smaller BERT architectures may be a computational cheaper choice to predict long-lived bug reports than larger architecture. }

\done{A possible venue for future research is to investigate other quantities and truncating methods (head-only, tail-only, head+tail) for extracting words from the description attribute. Another research direction is to investigate an end-to-end deep learning neural network predictor by performing the combination of fine-tuning on BERT pre-trained model and bug report structured fields.}
