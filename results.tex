\section{Results}\label{chapter:4:sec:results}

This section reports our experiments' results, and is organized according to the raised research questions (Section~\ref{chapter:4:sec:introduction}). 

\subsection {RQ1.What is the comparative accuracy of ML classifiers when predicting long-lived bugs using BERT-based feature extraction?}\label{result:rq1}

Figure~\ref{fig:rq1-all-balanced-accuracies-bert} shows the long-lived bug prediction performance of ML classifiers for the six projects. In this experiment, we considered the features extracted from the bug report's description using BERT. In the figure, we can observe that the SVM was the best in three datasets: 59.5\% in Freedesktop, 56.8\% in GCC, and 61.5\% in Mozilla. Also, we can see that the SVM was slightly worse than Random Forest~(RF) in Eclipse (57.4\% versus 57.6\%) and Gnome (56.4\% versus 56.6\%), and k-NN (57.3\% versus 57.9\%) in WineHQ.

\begin{figure*}[t!]
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Eclipse}
    \includegraphics[width=\textwidth]{figures/rq1-eclipse-balanced-accuracy.pdf}
    \label{fig:rq1-eclipse-balanced-accuracy}
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Freedesktop}
    \includegraphics[width=\textwidth]{figures/rq1-freedesktop-balanced-accuracy.pdf}
    \label{fig:rq1-freedesktop-balanced-accuracy}
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Gcc}
    \includegraphics[width=\textwidth]{figures/rq1-gcc-balanced-accuracy.pdf}
    \label{fig:rq1-gcc-balanced-accuracy} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Gnome}
    \includegraphics[width=\textwidth]{figures/rq1-gnome-balanced-accuracy.pdf}
    \label{fig:rq1-gnome-balanced-accuracy} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Mozilla}
    \includegraphics[width=\textwidth]{figures/rq1-mozilla-balanced-accuracy.pdf}
    \label{fig:rq1-mozilla-balanced-accuracy} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{WineHQ}
    \includegraphics[width=\textwidth]{figures/rq1-winehq-balanced-accuracy.pdf}
    \label{fig:rq1-winehq-balanced-accuracy.pdf} 
  \end{subfigure}
  \caption{Performance based on the balanced accuracy metric for all classifiers for all datasets. The solid yellow color highlights the best classifier when using the BERT-Based feature extraction for each dataset.}
  \label{fig:rq1-all-balanced-accuracies-bert}
\end{figure*}

Table~\ref{tab:rq1-balanced-accuracy-statistical-test} shows the statistical significance for each ML classifier's balanced accuracy pair yielded during the Repeated Cross-Validation process in the training step, as mentioned in the Methodology section. We can observe that the SVM classifier~(highlighted in blue) outperformed the others with statistical significance in many cases. 

\begin{table*}[ht!]
\centering
\caption{\textcolor{red}{2.13 - give more details about statistical test}The Wilcoxon Signed-Rank statistical significance for balanced accuracies in long-lived bug prediction. Each cell shows the results of a single classifier paired with the other. Left (`$\leftarrow$') and up (`$\uparrow$') arrows indicate the most accurate, while an `$-$' refers to ``no statistical difference between the pairs of classifiers" in the row and the column.}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} 
\hline
\multicolumn{2}{|c|}{} & knn & nb & nn & rf & svm &  & knn & nb & nn & rf & svm \\ 
\hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{\scriptsize{Eclipse}}} & knn & $-$ & $\uparrow$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} & \multirow{5}{*}{\rotatebox[origin=c]{90}{\scriptsize{Freedesktop}}} & $-$ & $\uparrow$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & nb & $\leftarrow$ & $-$ & $\leftarrow$ & $-$ & $-$ &  & $\leftarrow$ & $-$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & nn & $\uparrow$ & $\uparrow$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} &  & $\uparrow$ & $\uparrow$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & rf & $\leftarrow$ & $-$ & $\leftarrow$ & $-$ & \textbf{$-$} &  & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & svm & $\leftarrow$ & $-$ & $\leftarrow$ & $-$ & \textcolor{blue}{$-$} &  & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ \\ 
\hline
\multicolumn{1}{|l|}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\scriptsize{Gcc}}}} & knn & $-$ & $-$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} & \multicolumn{1}{l|}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\scriptsize{Gnome}}}} & $-$ & $-$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
\multicolumn{1}{|l|}{} & nb & $-$ & $-$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} & \multicolumn{1}{l|}{} & $-$ & $-$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
\multicolumn{1}{|l|}{} & nn & $-$ & $-$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} & \multicolumn{1}{l|}{} & $\uparrow$ & $\uparrow$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
\multicolumn{1}{|l|}{} & rf & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & \textcolor{blue}{$\uparrow$} & \multicolumn{1}{l|}{} & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & $-$ \\ 
\cline{2-7}\cline{9-13}
\multicolumn{1}{|l|}{} & svm & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & \multicolumn{1}{l|}{} & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & $-$ \\ 
\hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{\scriptsize{Mozilla}}} & knn & $-$ & $-$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} & \multirow{5}{*}{\rotatebox[origin=c]{90}{\scriptsize{WineHQ}}} & $-$ & $\uparrow$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & nb & $-$ & $-$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} &  & $\leftarrow$ & $-$ & $\leftarrow$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & nn & $\uparrow$ & $\uparrow$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} &  & $-$ & $\uparrow$ & $-$ & $\uparrow$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & rf & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & \textcolor{blue}{$\uparrow$} &  & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ & \textcolor{blue}{$\uparrow$} \\ 
\cline{2-7}\cline{9-13}
 & svm & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ &  & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $\leftarrow$ & $-$ \\
\hline
\end{tabular}
}
\label{tab:rq1-balanced-accuracy-statistical-test}
\end{table*}

\subsection {RQ2.What is the comparative accuracy of ML classifiers when predicting long-lived bugs using BERT-based feature extraction or TF-IDF-based feature extraction?}

Figure~\ref{fig:rq2-all-balanced-accuracies-bert-vs-tfidf} shows a comparison between the classifiers' accuracy performance when the long-lived bug prediction relies on features extracted from the bug report's description using BERT or TF-IDF. We can observe that the long-lived bug prediction's general performance using BERT-based feature extraction was systematically better than TF-IDF in all datasets. The most remarkable difference (9.4\%) between classifiers' performance occurred in the Mozilla dataset when SVM with BERT reaching 61.5\% and  Neural Network, 52.1\%. Only in Gnome, the Neural Network with TF-IDF was slightly better than this classifier with BERT.

\begin{figure*}[!t]
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Eclipse}
    \includegraphics[width=\textwidth]{figures/rq2-eclipse-balanced-accuracy.pdf}
    \label{fig:rq2-eclipse-balanced-accuracy}
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Freedesktop}
    \includegraphics[width=\textwidth]{figures/rq2-freedesktop-balanced-accuracy.pdf}
    \label{fig:rq2-freedesktop-balanced-accuracy}
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Gcc}
    \includegraphics[width=\textwidth]{figures/rq2-gcc-balanced-accuracy.pdf}
    \label{fig:rq2-gcc-balanced-accuracy} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Gnome}
    \includegraphics[width=\textwidth]{figures/rq2-gnome-balanced-accuracy.pdf}
    \label{fig:rq2-gnome-balanced-accuracy} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Mozilla}
    \includegraphics[width=\textwidth]{figures/rq2-mozilla-balanced-accuracy.pdf}
    \label{fig:rq2-mozilla-balanced-accuracy} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{WineHQ}
    \includegraphics[width=\textwidth]{figures/rq2-winehq-balanced-accuracy.pdf}
    \label{fig:rq2-winehq-balanced-accuracy} 
  \end{subfigure}
  \caption{The performance for all classifiers over all datasets based on the balanced accuracy metric, for both BERT- and TF-IDF-based feature extraction. The solid yellow color highlights the best classifier when using the BERT-based feature extraction, while the yellow hashed color highlights the best classifier when using the TF-IDF-based feature extraction for each dataset in the testing step.}
  \label{fig:rq2-all-balanced-accuracies-bert-vs-tfidf}
\end{figure*}

Figure~\ref{fig:rq2-all-difference-bert-vs-tfidf} summarizes the accuracy performance difference between ML classifiers using feature extraction based on BERT  and TF-IDF for all project datasets. The highest difference in favor of BERT was observed for Mozilla and the lowest, for Gnome.
 
 \begin{figure}[ht!]
     \centering
     \includegraphics[width=.70\textwidth]{figures/rq2-all-difference-bert-vs-tfidf.pdf}
     \caption{The performance difference between ML classifiers over investigated FLOSS when they used BERT- or TF-IDF-based feature extraction.}
     \label{fig:rq2-all-difference-bert-vs-tfidf}
 \end{figure}
 
 
\subsection{RQ3:}
Figure~\ref{fig:rq3-bert-ft-vs-tfidf} compares the balanced accuracy performance of the BERT variant's on the long-lived prediction task. We can note that the long-lived bug predictionâ€™s general performance using BERT variants in fine-tuning mode was systematically better than TF-IDF. The best performance by dataset was yielded by BERT$_{TINY}$ in Eclipse~(56.8\%), ELECTRA$_{BASE}$ in Freedesktop~(57.6\%), BERT$_{TINY}$ in GCC~(57.0\%), DISTILBERT in Gnome~(57.1\%), BERT$_{MINI}$ in Mozilla~(59.8\%), and BERT$_{SMALL}$ in WineHQ~(59.8\%). BERT variants had a performance worse than TF-IDF in four datasets: ALBERT$_BASE$ in Eclipse~(51.6\%) and GCC~(50.0\%), BERT$_{BASE}$ in Gnome~(54.9\%), and BERT$_{MEDIUM}$ in WineHQ~(53.7\%). Furthermore, the BERT variants with fine-tuning overcame the BERT$_{BASE}$ as feature extractor in 3 of 6 datasets: BERT$_{TINY}$ in GCC~(+0.7\%), DISTILBERT in GNOME~(+0.5\%), and BERT$_{SMALL}$ in WineHQ~(+3.2\%). Finally, we can observe that smaller BERT variants overcame the larger BERT variants in 5 of 6 datasets.
 
\begin{figure*}[t!]
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Eclipse}
    \includegraphics[width=\textwidth]{figures/Eclipse_FT_IDF_Metrics.jpg}
    \label{fig:rq3-bert-ft-vs-tfidf-eclipse}
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Freedesktop}
    \includegraphics[width=\textwidth]{figures/Freedesktop_FT_IDF_Metrics.jpg}
    \label{fig:rq3-bert-ft-vs-tfidf-freedesktop}
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Gcc}
    \includegraphics[width=\textwidth]{figures/GCC_FT_IDF_Metrics.jpg}
    \label{fig:rq3-bert-ft-vs-tfidf-gcc} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Gnome}
    \includegraphics[width=\textwidth]{figures/Gnome_FT_IDF_Metrics.jpg}
    \label{fig:rq3-bert-ft-vs-tfidf-gnome} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{Mozilla}
    \includegraphics[width=\textwidth]{figures/Mozilla_FT_IDF_Metrics.jpg}
    \label{fig:rq3-bert-ft-vs-tfidf-mozilla} 
  \end{subfigure}\hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \caption{WineHQ}
    \includegraphics[width=\textwidth]{figures/WineHQ_FT_IDF_Metrics.jpg}
    \label{fig:rq3-bert-ft-vs-tfidf-winehq} 
  \end{subfigure}
  \caption{The balanced accuracy performance for classifiers based on BERT variants to all datasets. The 
  red line indicates the best ML algorithm in performance using TD-IDF as feature extraction, and the blue line indicates the best ML algorithm in performance using BERT_{BASE}.}
  \label{fig:rq3-bert-ft-vs-tfidf}
\end{figure*}